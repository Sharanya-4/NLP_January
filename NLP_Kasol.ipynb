{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.1. Extract all twitter handles from following text. Twitter handle is the text that appears after https://twitter.com/ and is a single word. Also it contains only alpha numeric characters i.e. A-Z a-z , o to 9 and underscore _"
      ],
      "metadata": {
        "id": "Y312NYuuGF7Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBDPlrWKDe9B",
        "outputId": "014a899c-d8eb-4571-af90-be3e7ca60988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['elonmusk', 'teslarati', 'dummy_tesla', 'dummy_2_tesla']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = \"\"\"\n",
        "Follow our leader Elon musk on twitter here: https://twitter.com/elonmusk, more information\n",
        "on Tesla's products can be found at https://www.tesla.com/. Also here are leading influencers\n",
        "for tesla related news,\n",
        "https://twitter.com/teslarati\n",
        "https://twitter.com/dummy_tesla\n",
        "https://twitter.com/dummy_2_tesla\n",
        "\"\"\"\n",
        "sentence = r'https://twitter\\.com/([A-Za-z0-9_]+)'\n",
        "twitter = re.findall(sentence, text)\n",
        "print(twitter)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2. Extract Concentration Risk Types. It will be a text that appears after \"Concentration Risk:\", In below example, your regex should extract these two strings\n",
        "\n",
        "(1) Credit Risk\n",
        "\n",
        "(2) Supply Rish"
      ],
      "metadata": {
        "id": "ac30nMJgG1Xq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = '''\n",
        "Concentration of Risk: Credit Risk\n",
        "Financial instruments that potentially subject us to a concentration of credit risk consist of cash, cash equivalents, marketable securities,\n",
        "restricted cash, accounts receivable, convertible note hedges, and interest rate swaps. Our cash balances are primarily invested in money market funds\n",
        "or on deposit at high credit quality financial institutions in the U.S. These deposits are typically in excess of insured limits. As of September 30, 2021\n",
        "and December 31, 2020, no entity represented 10% or more of our total accounts receivable balance. The risk of concentration for our convertible note\n",
        "hedges and interest rate swaps is mitigated by transacting with several highly-rated multinational banks.\n",
        "Concentration of Risk: Supply Risk\n",
        "We are dependent on our suppliers, including single source suppliers, and the inability of these suppliers to deliver necessary components of our\n",
        "products in a timely manner at prices, quality levels and volumes acceptable to us, or our inability to efficiently manage these components from these\n",
        "suppliers, could have a material adverse effect on our business, prospects, financial condition and operating results.\n",
        "'''\n",
        "\n",
        "pattern = r'Concentration of Risk: (.*?)\\n'\n",
        "concentration_risks = re.findall(pattern, text)\n",
        "print(concentration_risks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sJoCsTtHAGn",
        "outputId": "ec12a8a2-55c3-49fa-e1d5-f5931989b305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Credit Risk', 'Supply Risk']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3. Companies in europe reports their financial numbers of semi annual basis and you can have a document like this. To exatract quarterly and semin annual period you can use a regex as shown below"
      ],
      "metadata": {
        "id": "G0NasfUeHTyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = '''\n",
        "Tesla's gross cost of operating lease vehicles in FY2021 Q1 was $4.85 billion.\n",
        "BMW's gross cost of operating vehicles in FY2021 S1 was $8 billion.\n",
        "'''\n",
        "\n",
        "pattern = r'FY(\\d{4} (?:Q[1-4]|S[1-2]))'\n",
        "matches = re.findall(pattern, text)\n",
        "\n",
        "print(\"Matches:\", matches)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19Tit8wcHUcg",
        "outputId": "7f9c210e-d82e-4ac8-fb8e-ae4c9d9c1e4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matches: ['2021 Q1', '2021 S1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1. Sentence & Word Tokenization In Spacy"
      ],
      "metadata": {
        "id": "z3G35JLPIAhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "d = nlp(\"This is NLP based Goggle Collab notebook \")\n",
        "for sentence in d.sents:\n",
        "    print(sentence)\n",
        "print(\"------------------------------------------\")\n",
        "for sentence in d.sents:\n",
        "    for word in sentence:\n",
        "        print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7vlR-6hIIip",
        "outputId": "97af8d00-a31c-4393-a847-7f561eebb757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is NLP based Goggle Collab notebook\n",
            "------------------------------------------\n",
            "This\n",
            "is\n",
            "NLP\n",
            "based\n",
            "Goggle\n",
            "Collab\n",
            "notebook\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2.Sentence & Word Tokenization In NLTK"
      ],
      "metadata": {
        "id": "Z_9KHYWKIkgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "sent_tokenize(\"This is NLP based Goggle Collab notebook\")\n",
        "word_tokenize(\"This is NLP based Goggle Collab notebook\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_Y96QHiIqYw",
        "outputId": "05dc48ba-0acf-4be7-9387-f20ef5bfbc6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'NLP', 'based', 'Goggle', 'Collab', 'notebook']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Collecting dataset websites from a book paragraph"
      ],
      "metadata": {
        "id": "4i_e1SO6Jook"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = '''\n",
        "Look for data to help you address the question. Governments are good\n",
        "sources because data from public research is often freely available. Good\n",
        "places to start include http://www.data.gov/, and http://www.science.\n",
        "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
        "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/,\n",
        "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
        "'''\n",
        "\n",
        "doc = nlp(text)\n",
        "data_websites = [token.text for token in doc if token.like_url]\n",
        "\n",
        "print(\"Data Websites:\")\n",
        "for website in data_websites:\n",
        "    print(website)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDBb33aeJoV1",
        "outputId": "20d5159d-1968-4802-bb84-503ca2c51d53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Websites:\n",
            "http://www.data.gov/\n",
            "http://www.science\n",
            "http://data.gov.uk/.\n",
            "http://www3.norc.org/gss+website/\n",
            "http://www.europeansocialsurvey.org/.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.1.\n",
        "Get all the proper nouns from a given text in a list and also count how many of them."
      ],
      "metadata": {
        "id": "DgoS-9srcsEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "text = '''Ravi and Raju are the best friends from school days. They wanted to go for a world tour and\n",
        "visit famous cities like Paris, London, Dubai, Rome etc. They also called their another friend Mohan to take part in this world tour.\n",
        "They started their journey from Hyderabad and spent the next 3 months travelling all the wonderful cities in the world and cherish happy moments!\n",
        "'''\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "\n",
        "proper_nouns = [token.text for token in doc if token.pos_ == \"PROPN\"]\n",
        "\n",
        "print(\"Proper Nouns:\", proper_nouns)\n",
        "print(\"Count:\", len(proper_nouns))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v81g5Iofcvkc",
        "outputId": "0303d2e8-c6a2-4a93-f070-03593a19aafd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proper Nouns: ['Raju', 'Paris', 'London', 'Dubai', 'Rome', 'Mohan', 'Hyderabad']\n",
            "Count: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.2.Get all companies names from a given text and also the count of them."
      ],
      "metadata": {
        "id": "OGOVmg_Tc-F9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "text = '''The Top 5 companies in USA are Tesla, Walmart, Amazon, Microsoft, Google and the top 5 companies in\n",
        "India are Infosys, Reliance, HDFC Bank, Hindustan Unilever and Bharti Airtel'''\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "\n",
        "company_names = [ent.text for ent in doc.ents if ent.label_ == 'ORG']\n",
        "\n",
        "print(\"Company Names:\", company_names)\n",
        "print(\"Count:\", len(company_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gz3pOvdUc_CI",
        "outputId": "402e6461-ff3e-4c2f-e607-00e805a25d05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Company Names: ['Tesla', 'Walmart', 'Amazon', 'Microsoft', 'Google', 'Infosys', 'Reliance', 'HDFC Bank', 'Hindustan Unilever', 'Bharti']\n",
            "Count: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.1.Stemming in NLTK"
      ],
      "metadata": {
        "id": "EcFAI96zdQm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "words = [\"eating\", \"eats\", \"eat\", \"ate\", \"adjustable\", \"rafting\", \"ability\", \"meeting\"]\n",
        "\n",
        "stems = [(word, stemmer.stem(word)) for word in words]\n",
        "\n",
        "for original, stemmed in stems:\n",
        "    print(original, \",\", stemmed)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXR_56C4dRwM",
        "outputId": "e16217f8-41a1-4a1e-fb8d-cf3399b906c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating , eat\n",
            "eats , eat\n",
            "eat , eat\n",
            "ate , ate\n",
            "adjustable , adjust\n",
            "rafting , raft\n",
            "ability , abil\n",
            "meeting , meet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.2.Lemmatization in Spacy"
      ],
      "metadata": {
        "id": "UJjG0y7TdkIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "text = \"The quick brown foxes are jumping over the lazy dogs\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(f\"{token.text} | {token.lemma_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrUe4rY0dlb-",
        "outputId": "7322be2f-957e-4172-cfac-b9d05f3d02b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The | the\n",
            "quick | quick\n",
            "brown | brown\n",
            "foxes | fox\n",
            "are | be\n",
            "jumping | jump\n",
            "over | over\n",
            "the | the\n",
            "lazy | lazy\n",
            "dogs | dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.3. Customizing lemmatizer"
      ],
      "metadata": {
        "id": "P1ppKCMmd53T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ar = nlp.get_pipe('attribute_ruler')\n",
        "doc = nlp(\"The quick brown foxes are jumping over the lazy dogs\")\n",
        "for token in doc:\n",
        "    print(token.text, \"|\", token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-srxuBv9eBTm",
        "outputId": "1b3b4fa0-df98-4575-bb1c-12d404a7891e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The | the\n",
            "quick | quick\n",
            "brown | brown\n",
            "foxes | fox\n",
            "are | be\n",
            "jumping | jump\n",
            "over | over\n",
            "the | the\n",
            "lazy | lazy\n",
            "dogs | dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.1.convert the given text into it's base form using both stemming and lemmatization."
      ],
      "metadata": {
        "id": "JxWUzAKTfHT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "import spacy\n",
        "\n",
        "\n",
        "text = \"\"\"Latha is very multi talented girl. She is good at many skills like dancing, running, singing, playing.\n",
        "She also likes eating Pav Bhagi. She has a habit of fishing and swimming too. Besides all this, she is wonderful at cooking too.\n",
        "\"\"\"\n",
        "doc = nlp(text)\n",
        "stemmed_text_nltk = ' '.join([stemmer.stem(word) for word in text.split()])\n",
        "lemmatized_text_spacy = ' '.join([token.lemma_ for token in doc])\n",
        "\n",
        "print(\"\\nStemmed Text (NLTK):\\n\", stemmed_text_nltk)\n",
        "print(\"\\nLemmatized Text (spaCy):\\n\", lemmatized_text_spacy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyCuvAyxfIUC",
        "outputId": "6da81131-bb54-475a-ac3c-36e19299e9a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stemmed Text (NLTK):\n",
            " latha is veri multi talent girl. she is good at mani skill like dancing, running, singing, playing. she also like eat pav bhagi. she ha a habit of fish and swim too. besid all this, she is wonder at cook too.\n",
            "\n",
            "Lemmatized Text (spaCy):\n",
            " Latha be very multi talented girl . she be good at many skill like dancing , running , singing , play . \n",
            " she also like eat Pav Bhagi . she have a habit of fishing and swim too . besides all this , she be wonderful at cook too . \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.1.POS tags"
      ],
      "metadata": {
        "id": "V0D6E0gvfkiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Elon flew to mars yesterday. He carried biryani masala with him\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token,\" | \", token.pos_, \" | \", spacy.explain(token.pos_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgCIwHNvfq0T",
        "outputId": "85ce30ba-c0b4-43db-9ea4-79762d78112d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elon  |  PROPN  |  proper noun\n",
            "flew  |  VERB  |  verb\n",
            "to  |  ADP  |  adposition\n",
            "mars  |  NOUN  |  noun\n",
            "yesterday  |  NOUN  |  noun\n",
            ".  |  PUNCT  |  punctuation\n",
            "He  |  PRON  |  pronoun\n",
            "carried  |  VERB  |  verb\n",
            "biryani  |  ADJ  |  adjective\n",
            "masala  |  NOUN  |  noun\n",
            "with  |  ADP  |  adposition\n",
            "him  |  PRON  |  pronoun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "7.2.Removing all SPACE, PUNCT and X token from text"
      ],
      "metadata": {
        "id": "ZFL3gFzjfyh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"\"\"Microsoft Corp. today announced the following results for the quarter ended December 31, 2021, as compared to the corresponding period of last fiscal year:\n",
        "\n",
        "· Revenue was $51.7 billion and increased 20%\n",
        "· Operating income was $22.2 billion and increased 24%\n",
        "· Net income was $18.8 billion and increased 21%\n",
        "· Diluted earnings per share was $2.48 and increased 22%\n",
        "“Digital technology is the most malleable resource at the world’s disposal to overcome constraints and reimagine everyday work and life,” said Satya Nadella, chairman and chief executive officer of Microsoft. “As tech as a percentage of global GDP continues to increase, we are innovating and investing across diverse and growing markets, with a common underlying technology stack and an operating model that reinforces a common strategy, culture, and sense of purpose.”\n",
        "“Solid commercial execution, represented by strong bookings growth driven by long-term Azure commitments, increased Microsoft Cloud revenue to $22.1 billion, up 32% year over year” said Amy Hood, executive vice president and chief financial officer of Microsoft.\"\"\"\n",
        "\n",
        "\n",
        "doc = nlp(text)\n",
        "filtered_tokens = [token.text for token in doc if token.is_alpha]\n",
        "cleaned_text = ' '.join(filtered_tokens)\n",
        "print(filtered_tokens[:10])\n",
        "print(\"\\nCleaned Text:\\n\", cleaned_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOF5EzKBf0Rf",
        "outputId": "aff73fe0-f029-4193-f982-3d4cd2c4d70f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Microsoft', 'today', 'announced', 'the', 'following', 'results', 'for', 'the', 'quarter', 'ended']\n",
            "\n",
            "Cleaned Text:\n",
            " Microsoft today announced the following results for the quarter ended December as compared to the corresponding period of last fiscal year Revenue was billion and increased Operating income was billion and increased Net income was billion and increased Diluted earnings per share was and increased Digital technology is the most malleable resource at the world disposal to overcome constraints and reimagine everyday work and life said Satya Nadella chairman and chief executive officer of Microsoft As tech as a percentage of global GDP continues to increase we are innovating and investing across diverse and growing markets with a common underlying technology stack and an operating model that reinforces a common strategy culture and sense of purpose Solid commercial execution represented by strong bookings growth driven by long term Azure commitments increased Microsoft Cloud revenue to billion up year over year said Amy Hood executive vice president and chief financial officer of Microsoft\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = doc.count_by(spacy.attrs.POS)\n",
        "for k,v in count.items():\n",
        "    print(doc.vocab[k].text, \"|\",v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F4YIhwHfzjx",
        "outputId": "fabe2bf3-9f80-4675-d25d-082196dba6fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROPN | 13\n",
            "NOUN | 46\n",
            "VERB | 23\n",
            "DET | 9\n",
            "ADP | 16\n",
            "NUM | 16\n",
            "PUNCT | 27\n",
            "SCONJ | 1\n",
            "ADJ | 21\n",
            "SPACE | 6\n",
            "AUX | 6\n",
            "SYM | 5\n",
            "CCONJ | 12\n",
            "ADV | 3\n",
            "PART | 3\n",
            "PRON | 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.1.Extract all the Geographical (cities, Countries, states) names from a given text (NER)"
      ],
      "metadata": {
        "id": "cLaYGOh9gaiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"\"\"Kiran wants to know the famous foods in each state of India. So, he opened Google and searched for this question. Google showed that\n",
        "in Delhi it is Chaat, in Gujarat it is Dal Dhokli, in Tamilnadu it is Pongal, in Andhra Pradesh it is Biryani, in Assam it is Papaya Khar,\n",
        "in Bihar it is Litti Chowkha and so on for all other states.\"\"\"\n",
        "\n",
        "doc = nlp(text)\n",
        "geographical_entities = [ent.text for ent in doc.ents if ent.label_ == 'GPE']\n",
        "\n",
        "print(\"Geographical Entities (Countries):\", geographical_entities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj0MTtKLgcNO",
        "outputId": "54882611-0da3-4c82-cebc-1c7c377d9733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Geographical Entities (Countries): ['India', 'Delhi', 'Gujarat', 'Tamilnadu', 'Pongal', 'Andhra', 'Assam', 'Bihar']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.2.Extract all the birth dates of cricketers in the given Text"
      ],
      "metadata": {
        "id": "foG9ApBGgvMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"\"\"Sachin Tendulkar was born on 24 April 1973, Virat Kohli was born on 5 November 1988, Dhoni was born on 7 July 1981\n",
        "and finally Ricky Ponting was born on 19 December 1974.\"\"\"\n",
        "\n",
        "doc = nlp(text)\n",
        "birth_dates = [ent.text for ent in doc.ents if ent.label_ == 'DATE']\n",
        "\n",
        "print(\"Birth Dates of Cricketers:\", birth_dates)\n",
        "print(\"Count:\", len(birth_dates))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Stzi7WvgwO_",
        "outputId": "9c3ec973-70f0-4e4b-8964-d47352ca3125"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Birth Dates of Cricketers: ['24 April 1973', '5 November 1988', '7 July 1981', '19 December 1974']\n",
            "Count: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.1.Stop Words: From a Given Text, Count the number of stop words in it.\n",
        "Print the percentage of stop word tokens compared to all tokens in a given text."
      ],
      "metadata": {
        "id": "aMEDxsRNhf1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy_loggers\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"\"\"This is an example text to count the number of stop words. It contains some common stop words like 'the', 'is', 'and', etc.\"\"\"\n",
        "\n",
        "doc = nlp(text)\n",
        "stop_word_count = sum([1 for token in doc if token.is_stop])\n",
        "total_tokens = len(doc)\n",
        "percentage_stop_words = (stop_word_count / total_tokens) * 100\n",
        "\n",
        "# Print the results\n",
        "print(\"Number of Stop Words:\", stop_word_count)\n",
        "print(\"Total Tokens:\", total_tokens)\n",
        "print(\"Percentage of Stop Words:\", percentage_stop_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9z6sp3whxaI",
        "outputId": "66ade34a-c753-4c34-dcb0-f24427d07aa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Stop Words: 11\n",
            "Total Tokens: 34\n",
            "Percentage of Stop Words: 32.35294117647059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.2.Spacy default implementation considers \"not\" as a stop word. But in some scenarios removing 'not' will completely change the meaning of the statement/text."
      ],
      "metadata": {
        "id": "ubLC0h26iYrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    doc = nlp(text)\n",
        "    no_stop_words = ' '.join([token.text for token in doc if not token.is_stop])\n",
        "    return no_stop_words\n",
        "\n",
        "nlp.vocab['not'].is_stop = False\n",
        "\n",
        "positive_text = preprocess('this is a good movie')\n",
        "negative_text = preprocess('this is not a good movie')\n",
        "\n",
        "print(\"Transformed Text 1:\", positive_text)\n",
        "print(\"Transformed Text 2:\", negative_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Er9aRB4fiWjW",
        "outputId": "d42d94d3-631d-4bb2-9ec1-a12516ac170e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformed Text 1: good movie\n",
            "Transformed Text 2: not good movie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.3.From a given text, output the most frequently used token after removing all the stop word tokens and punctuations in it"
      ],
      "metadata": {
        "id": "ACORFRmji0YB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text1 = \"\"\"The India men's national cricket team, also known as the Men in Blue, represents India in men's international cricket.\n",
        "It is governed by the Board of Control for Cricket in India (BCCI), and is a Full Member of the International Cricket Council (ICC) with Test,\n",
        "One Day International (ODI) and Twenty20 International (T20I) status. Cricket was introduced to India by British sailors in the 18th century, and the\n",
        "first cricket club was established in 1792. India's national cricket team played its first Test match on 25 June 1932 at Lord's, becoming the sixth team to be\n",
        "granted test cricket status.\"\"\"\n",
        "\n",
        "text = text1.lower()\n",
        "doc = nlp(text)\n",
        "filtered_tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "most_frequent_token = Counter(filtered_tokens).most_common(1)\n",
        "result = most_frequent_token[0][0] if most_frequent_token else None\n",
        "\n",
        "print(\"Most Frequently Used Token:\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vy_OeLzi3J-",
        "outputId": "a81fefdb-7ed9-4438-bcd8-f80d50cc2092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Frequently Used Token: cricket\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.1. Word_Vector Spacy"
      ],
      "metadata": {
        "id": "cz_6F6gPkHui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "doc = nlp(\"dog cat banana kem\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, \"Vector:\", token.has_vector, \"OOV:\", token.is_oov)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7ce23SbkOm-",
        "outputId": "33f93fa8-96dc-4b2b-9ac5-c09ff5258433"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dog Vector: True OOV: True\n",
            "cat Vector: True OOV: True\n",
            "banana Vector: True OOV: True\n",
            "kem Vector: True OOV: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc.vector.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8y2dJSoHkWuT",
        "outputId": "e7360655-fd5c-49f7-90ca-2d14bd822734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(96,)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_token = nlp(\"bread\")\n",
        "doc = nlp(\"bread sandwich burger car tiger human wheat\")\n",
        "for token in doc:\n",
        "    print(f\"{token.text} <-> {base_token.text}:\", token.similarity(base_token))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2c6v7kjkd60",
        "outputId": "37a3e741-d778-4f3b-e474-e4c1a37d0be4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bread <-> bread: 0.4868319181235256\n",
            "sandwich <-> bread: 0.26677175290569444\n",
            "burger <-> bread: 0.2403758463416168\n",
            "car <-> bread: 0.21232416060990508\n",
            "tiger <-> bread: 0.416205187946548\n",
            "human <-> bread: 0.17155441719759282\n",
            "wheat <-> bread: 0.6217515964505436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-57-f1567fd182bd>:4: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(f\"{token.text} <-> {base_token.text}:\", token.similarity(base_token))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_similarity(base_word, words_to_compare):\n",
        "    base_token = nlp(base_word)\n",
        "    doc = nlp(words_to_compare)\n",
        "    for token in doc:\n",
        "        print(f\"{token.text} <-> {base_token.text}: \", token.similarity(base_token))\n",
        "print_similarity(\"iphone\", \"apple samsung iphone dog kitten\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udr_SNTMkmH3",
        "outputId": "f97ca8eb-60af-4f00-91e8-1aebf39fb545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apple <-> iphone:  0.26182551438999324\n",
            "samsung <-> iphone:  0.05472672518057898\n",
            "iphone <-> iphone:  0.30389348669405114\n",
            "dog <-> iphone:  0.4053742132890232\n",
            "kitten <-> iphone:  0.34180785111852907\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-58-5e1ef92398a6>:5: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(f\"{token.text} <-> {base_token.text}: \", token.similarity(base_token))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.1.Word Vectors Overview Using Gensim Library"
      ],
      "metadata": {
        "id": "1A8Q4tkOkxR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "# This is a huge model (~1.6 gb) and it will take some time to load\n",
        "\n",
        "wv = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKBjQLrrkyVD",
        "outputId": "c2237416-d571-46d5-ce31-45adcd24f2a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wv.similarity(w1=\"great\", w2=\"good\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO8tk8lLtMn8",
        "outputId": "c660028b-8da8-4c0c-ae87-2f08adf4736b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.729151"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "wv.most_similar(\"good\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jry0TkMetOzO",
        "outputId": "9620a5de-94fb-4544-ec31-c539f9cfb760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('great', 0.7291510105133057),\n",
              " ('bad', 0.7190051078796387),\n",
              " ('terrific', 0.6889115571975708),\n",
              " ('decent', 0.6837348341941833),\n",
              " ('nice', 0.6836092472076416),\n",
              " ('excellent', 0.644292950630188),\n",
              " ('fantastic', 0.6407778263092041),\n",
              " ('better', 0.6120728850364685),\n",
              " ('solid', 0.5806034803390503),\n",
              " ('lousy', 0.576420247554779)]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wv.most_similar(\"dog\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SJ0ebFltQyj",
        "outputId": "13de8747-59d1-44a2-e574-b7f09250897c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('dogs', 0.8680489659309387),\n",
              " ('puppy', 0.8106428384780884),\n",
              " ('pit_bull', 0.780396044254303),\n",
              " ('pooch', 0.7627376914024353),\n",
              " ('cat', 0.7609457969665527),\n",
              " ('golden_retriever', 0.7500901818275452),\n",
              " ('German_shepherd', 0.7465174198150635),\n",
              " ('Rottweiler', 0.7437615394592285),\n",
              " ('beagle', 0.7418621778488159),\n",
              " ('pup', 0.740691065788269)]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "739yisUmtTG6",
        "outputId": "3c047e37-7d99-472a-acfd-8d9ea93623ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('queen', 0.7118193507194519),\n",
              " ('monarch', 0.6189674139022827),\n",
              " ('princess', 0.5902431011199951),\n",
              " ('crown_prince', 0.5499460697174072),\n",
              " ('prince', 0.5377321839332581)]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wv.most_similar(positive=['france', 'berlin'], negative=['paris'], topn=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_39AdE_tU_c",
        "outputId": "27082d80-d0bf-474c-9134-acb4b55910f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('germany', 0.5094343423843384),\n",
              " ('european', 0.48650455474853516),\n",
              " ('german', 0.4714890420436859),\n",
              " ('austria', 0.46964022517204285),\n",
              " ('swedish', 0.4645182490348816)]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wv.doesnt_match([\"facebook\", \"cat\", \"google\", \"microsoft\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "AyCch3MPtZ7v",
        "outputId": "303d7012-919a-4374-8f44-d3457b5cac7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "glv = api.load(\"glove-twitter-25\")\n",
        "glv.most_similar(\"good\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dweiQ2Fitc7l",
        "outputId": "c2f1f1ef-2f64-4a5c-dcdf-667a8fc44fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('too', 0.9648017287254333),\n",
              " ('day', 0.9533665180206299),\n",
              " ('well', 0.9503170847892761),\n",
              " ('nice', 0.9438973665237427),\n",
              " ('better', 0.9425962567329407),\n",
              " ('fun', 0.9418926239013672),\n",
              " ('much', 0.9413353800773621),\n",
              " ('this', 0.9387555122375488),\n",
              " ('hope', 0.9383506774902344),\n",
              " ('great', 0.9378516674041748)]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glv.doesnt_match(\"breakfast cereal dinner lunch\".split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "r85spBEztfKS",
        "outputId": "137e0f87-0479-4f73-ae6d-63deab1dbb2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cereal'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glv.doesnt_match(\"facebook cat google microsoft\".split())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "TBbpXfuvthKZ",
        "outputId": "8bbca630-5fcc-4372-d52d-9619cf6d64d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glv.doesnt_match(\"banana grapes orange human\".split())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-VqHE5mRtkMF",
        "outputId": "0970a6be-db2d-42d3-a00d-d2f0e8fbe5a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'human'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    }
  ]
}